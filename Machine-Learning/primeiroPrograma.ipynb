{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor() # definido a conversão de imagem para o tensor\n",
    "\n",
    "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # Carrega a parte de treino do dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
    "\n",
    "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) # Carrega a parte de validação do dataset\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False) # Cria um buffer para pegar os dados por partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x236a4ea9640>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "imagens, etiquetas = dataiter.__next__()\n",
    "plt.imshow(imagens[1].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(imagens[0].shape) # para verificar as dimensões do tensor de cada imagem\n",
    "print(etiquetas[0].shape) # para  verificar as dimensões do tensor de cada etiqueta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelo(nn.Module):\n",
    "    def  __init__(self):\n",
    "        super(Modelo, self).__init__()\n",
    "        self.linear1=nn.Linear(28*28, 128) # camada de entrada, 784 neurônios que se ligam a 128\n",
    "        self.linear2=nn.Linear(128, 64) # camada interna 1, 128 neurônios que se ligam a 64\n",
    "        self.linear3=nn.Linear(64, 10) # camada interna 2, 64 neurônios que se ligam a 10\n",
    "        # Para a camada de saida não e necessario definir nada pois só precisamos pegar o output da camada interna 2\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a camada interna 1\n",
    "        X = F.relu(self.linear1(X)) # função de ativação da camada interna 1 para a camada interna 2\n",
    "        X = self.linear3(X) # função de ativação da camada interna 1 para a camada de saída, nesse caso f(x) = x\n",
    "        return F.log_softmax(X, dim=1) # dados utilizados para calcular a perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(modelo, trainloader, device):\n",
    "\n",
    "    otimizador = optim.SGD(modelo.parameters(),  lr=0.01, momentum=0.5) # define a politica de atualização dos pesos e da bias\n",
    "    inicio = time() # timer para sabermos quanto tempo levou o treino\n",
    "\n",
    "    criterio = nn.NLLLoss() # definido o criterio para calcular a perda\n",
    "    EPOCHS = 10 # numero de epochs que o algoritmo rodará\n",
    "    modelo.train() # ativando o modo de treinamento do modelo\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
    "\n",
    "        for images, etiquetas in trainloader:\n",
    "            \n",
    "            images = images.view(images.shape[0], -1) # convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compativeis com a rede neural\n",
    "            otimizador.zero_grad() # zerando os  gradientes por conta do ciclo anterior\n",
    "\n",
    "            output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
    "\n",
    "            perda_instantanea.backward() # back propagation a partir da perda\n",
    "\n",
    "            otimizador.step() # atualizando os pesos e bias\n",
    "\n",
    "            perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
    "\n",
    "        else:\n",
    "            print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
    "    print(\"\\nTempo de treino (em minutos) =\",(time()-inicio)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao(modelo, valloader, device):\n",
    "    conta_corretas, conta_todas = 0, 0\n",
    "    for imagens,etiquetas in valloader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = imagens[i].view(1,784)\n",
    "            # desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
    "            with torch.no_grad():\n",
    "                logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
    "            \n",
    "            ps = torch.exp(logps) # converte output para escala normal(lembrando que é um tensor)\n",
    "            probab = list(ps.cpu().numpy()[0])\n",
    "            etiqueta_pred = probab.index(max(probab)) # converte o tensor em número, no caso, o número que o modelo previu como correto\n",
    "            etiqueta_certa = etiquetas.numpy()[i]\n",
    "            if(etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto\n",
    "                conta_corretas += 1\n",
    "            conta_todas += 1\n",
    "\n",
    "    print(\"Total de imagens testadas =\", conta_todas)\n",
    "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modelo(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = Modelo()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" ) \n",
    "modelo.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
